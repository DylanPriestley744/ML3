# ============================================================
# K 折交叉验证 + ResNet18 预训练 + TTA + 概率集成
# 目录结构:
# /kaggle/input/mltask3/fer_data/fer_data/train
# /kaggle/input/mltask3/fer_data/fer_data/test
#
# 标签:
# 0: Angry, 1: Fear, 2: Happy, 3: Sad, 4: Surprise, 5: Neutral
# ============================================================

import os
import glob
import random
import numpy as np
import pandas as pd

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models

from sklearn.model_selection import StratifiedKFold

# ------------------ 配置区 ------------------
CONFIG = {
    "data_root": "/kaggle/input/mltask3/fer_data/fer_data",

    "n_folds": 5,                # K 折
    "batch_size": 128,
    "num_epochs": 25,            # ★ epoch 上限，提高到 25
    "lr": 3e-4,
    "num_workers": 2,
    "seed": 42,

    "use_class_weights": True,   # 类别不平衡时建议 True
    "early_stopping_patience": 6,# ★ 连续 6 轮 val_acc 不涨就早停

    "strip_test_ext": False,     # 如果提交格式要求 ID 不带 .jpg，就改 True
}
# -------------------------------------------------------


def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


set_seed(CONFIG["seed"])

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))


# ============================================================
# 1. 读取训练集路径 & 标签
# ============================================================
DATA_ROOT = CONFIG["data_root"]
TRAIN_DIR = os.path.join(DATA_ROOT, "train")
TEST_DIR = os.path.join(DATA_ROOT, "test")

print("TRAIN_DIR:", TRAIN_DIR)
print("TEST_DIR :", TEST_DIR)
print("Train classes:", os.listdir(TRAIN_DIR))

# 文件夹名 -> Kaggle 标签
kaggle_label_map = {
    "Angry": 0,
    "Fear": 1,
    "Happy": 2,
    "Sad": 3,
    "Surprise": 4,
    "Neutral": 5,
}
class_names = sorted(kaggle_label_map.keys())
print("Class names:", class_names)

image_paths = []
labels = []

exts = ("*.jpg", "*.jpeg", "*.png")

for class_name in class_names:
    class_dir = os.path.join(TRAIN_DIR, class_name)
    files = []
    for ext in exts:
        files.extend(glob.glob(os.path.join(class_dir, ext)))
    files = sorted(files)
    print(class_name, "num images:", len(files))
    image_paths.extend(files)
    labels.extend([kaggle_label_map[class_name]] * len(files))

image_paths = np.array(image_paths)
labels = np.array(labels)
num_classes = len(kaggle_label_map)

print("Total train images:", len(image_paths))


# ============================================================
# 2. 类别权重（缓解不平衡）
# ============================================================
if CONFIG["use_class_weights"]:
    label_counts = np.bincount(labels, minlength=num_classes)
    print("Label counts:", label_counts)
    inv_freq = 1.0 / label_counts
    class_weights = inv_freq / inv_freq.mean()
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    print("Class weights:", class_weights)
else:
    class_weights = None


# ============================================================
# 3. Dataset & Transform
# ============================================================
imagenet_mean = [0.485, 0.456, 0.406]
imagenet_std  = [0.229, 0.224, 0.225]

train_transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize(256),
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),
])

val_test_transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),
])


class FaceEmotionDataset(Dataset):
    def __init__(self, paths, labels, transform):
        self.paths = list(paths)
        self.labels = list(labels)
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        path = self.paths[idx]
        img = Image.open(path).convert("L")
        if self.transform is not None:
            img = self.transform(img)
        label = self.labels[idx]
        return img, label


class TestImageDataset(Dataset):
    def __init__(self, img_dir, transform=None):
        paths = []
        for ext in exts:
            paths.extend(glob.glob(os.path.join(img_dir, ext)))
        self.img_paths = sorted(paths)
        self.ids = [os.path.basename(p) for p in self.img_paths]
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        path = self.img_paths[idx]
        img = Image.open(path).convert("L")
        if self.transform is not None:
            img = self.transform(img)
        return img, self.ids[idx]


test_dataset = TestImageDataset(TEST_DIR, transform=val_test_transform)
test_loader = DataLoader(
    test_dataset,
    batch_size=256,
    shuffle=False,
    num_workers=CONFIG["num_workers"],
    pin_memory=True,
)
print("Num test images:", len(test_dataset))


# ============================================================
# 4. 模型 & 损失 & 训练函数
# ============================================================
def build_model(num_classes: int):
    try:
        model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
    except Exception:
        model = models.resnet18(pretrained=True)
    in_features = model.fc.in_features
    model.fc = nn.Linear(in_features, num_classes)
    return model


def get_criterion():
    try:
        criterion = nn.CrossEntropyLoss(
            weight=class_weights,
            label_smoothing=0.05,
        )
    except TypeError:
        criterion = nn.CrossEntropyLoss(
            weight=class_weights,
        )
    return criterion


def train_one_epoch(model, loader, optimizer, criterion, device):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in loader:
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * images.size(0)
        _, preds = torch.max(outputs, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return running_loss / total, correct / total


def evaluate(model, loader, criterion, device):
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item() * images.size(0)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    return running_loss / total, correct / total


# ============================================================
# 5. K 折训练 + 每折 TTA 预测测试集 + 概率集成
# ============================================================
n_folds = CONFIG["n_folds"]
skf = StratifiedKFold(
    n_splits=n_folds,
    shuffle=True,
    random_state=CONFIG["seed"],
)

num_test = len(test_dataset)
all_test_probs = np.zeros((num_test, num_classes), dtype=np.float32)

for fold, (train_idx, val_idx) in enumerate(skf.split(image_paths, labels), start=1):
    print(f"\n========== Fold {fold}/{n_folds} ==========")

    train_paths = image_paths[train_idx]
    train_labels = labels[train_idx]
    val_paths = image_paths[val_idx]
    val_labels = labels[val_idx]

    train_ds = FaceEmotionDataset(train_paths, train_labels, train_transform)
    val_ds = FaceEmotionDataset(val_paths, val_labels, val_test_transform)

    train_loader = DataLoader(
        train_ds,
        batch_size=CONFIG["batch_size"],
        shuffle=True,
        num_workers=CONFIG["num_workers"],
        pin_memory=True,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=CONFIG["batch_size"],
        shuffle=False,
        num_workers=CONFIG["num_workers"],
        pin_memory=True,
    )

    model = build_model(num_classes).to(device)
    criterion = get_criterion()
    optimizer = optim.AdamW(
        model.parameters(),
        lr=CONFIG["lr"],
        weight_decay=1e-4,
    )
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=0.5, patience=2, verbose=True
    )

    # ------- 早停逻辑：按 val_acc -------
    best_val_acc = 0.0
    best_model_path = f"best_resnet18_fold{fold}.pth"
    patience = CONFIG["early_stopping_patience"]
    no_improve = 0

    for epoch in range(1, CONFIG["num_epochs"] + 1):
        train_loss, train_acc = train_one_epoch(
            model, train_loader, optimizer, criterion, device
        )
        val_loss, val_acc = evaluate(model, val_loader, criterion, device)

        # 学习率调度器仍然用 val_loss
        scheduler.step(val_loss)

        print(
            f"Fold {fold} | Epoch [{epoch}/{CONFIG['num_epochs']}] "
            f"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | "
            f"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}"
        )

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            no_improve = 0
            torch.save(model.state_dict(), best_model_path)
            print(f"  => Best model saved (val_acc={best_val_acc:.4f}).")
        else:
            no_improve += 1
            print(f"  No improvement (val_acc) for {no_improve} epoch(s).")

        if no_improve >= patience:
            print("  Early stopping triggered for this fold.")
            break

    # ---------- 用本折最优模型 + TTA 预测测试集 ----------
    model.load_state_dict(torch.load(best_model_path, map_location=device))
    model.eval()

    fold_probs = []

    with torch.no_grad():
        for images, _ids in test_loader:
            images = images.to(device, non_blocking=True)

            # 原图
            outputs_orig = model(images)
            # 水平翻转
            images_flip = torch.flip(images, dims=[3])
            outputs_flip = model(images_flip)

            # logits 平均后做 softmax
            outputs = (outputs_orig + outputs_flip) / 2.0
            probs = torch.softmax(outputs, dim=1).cpu().numpy()

            fold_probs.append(probs)

    fold_probs = np.vstack(fold_probs)
    all_test_probs += fold_probs
    print(f"  Fold {fold} test predictions added.")


# ============================================================
# 6. 平均所有折的概率 & 生成提交文件
# ============================================================
all_test_probs /= n_folds
final_preds = all_test_probs.argmax(axis=1)

ids_for_csv = test_dataset.ids
if CONFIG["strip_test_ext"]:
    ids_for_csv = [os.path.splitext(x)[0] for x in ids_for_csv]

submission = pd.DataFrame({
    "ID": ids_for_csv,
    "Emotion": final_preds,
})

print(submission.head())
submission.to_csv("submission.csv", index=False)
print("Saved submission.csv")
